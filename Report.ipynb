{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification using GoogLeNet Architecture from Scratch\n",
    "\n",
    "#### In this notebook we are trying to make Object Classification using CNN like [GoogleNet](https://arxiv.org/pdf/1409.4842.pdf).\n",
    "- We have used **GOOGLE CLOUD Platform** to test and train our model\n",
    "- We have also used image augmentation to boost the performance of deep networks.\n",
    "- Due to overfitting of the model and getting less accuracy in the [Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) of Dog (Subset of Imagenet Dataset) in our CNN model so we also try to other CNN and run on CIFAR-10 and got 68% accuracy in 50 Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to setup GCP(Google Cloud Platform) for Keras and Tensorflow\n",
    "\n",
    "1. Setup Virtual Machine Instance in GCP using [link](https://cloud.google.com/compute/docs/instances/). Make sure your instance have GPU.\n",
    "- Follow all the steps in [link](https://medium.com/google-cloud/running-jupyter-notebooks-on-gpu-on-google-cloud-d44f57d22dbd) to setup Anaconda, Tensorflow and Keras with GPU driver.\n",
    "- To import dataset to VM use ssh to your VM run:\n",
    "    - `gcloud compute scp ~/localdirectory/ example-instance:~/destinationdirectory`\n",
    "- Navigate to you jupyter on local browser and make a new notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some more library to install to run the notebook:**\n",
    "- Install PIL : pip install pillow --> Python Imaging Library which adds image processing capabilities to your python interpreter.\n",
    "\n",
    "- Install tqdm  : pip install tqdm --> tqdm is used to show progress bar\n",
    "- Install h5py  : pip install h5py --> used to store weights in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import library\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,GlobalAveragePooling2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import model_from_json\n",
    "\n",
    "#pre-processing Images\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True \n",
    "\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allow the GPU as memory is needed rather than pre-allocate memory\n",
    "- You can find more details of tensorflow GPU [here](https://www.tensorflow.org/programmers_guide/tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to load dataset\n",
    "def load_dataset(path):\n",
    "    #load files from path\n",
    "    data = load_files(path)\n",
    "    #takes the filename and put in array\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    #one hot encoding\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_files, train_targets = load_dataset('Dog/train')\n",
    "valid_files, valid_targets = load_dataset('Dog/valid')\n",
    "test_files, test_targets = load_dataset('Dog/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Affenpinscher',\n",
       " 'Afghan_hound',\n",
       " 'Airedale_terrier',\n",
       " 'Akita',\n",
       " 'Alaskan_malamute']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just getting first 5 dogs breads\n",
    "dog_names = [item.split('.')[1].rstrip('\\/') for item in sorted(glob(\"Dog/train/*/\"))]\n",
    "dog_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8341 total dog images.\n",
      "\n",
      "There are 6670 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This dataset is already split into train, validation and test parts. As the traning set consits of 6670 images, there are only 50 dogs per breed on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PreProcess the Data\n",
    "- Path_to_tensor is the function that takes the image path, convert into array and return the 4D tensor with shape (1,224,224,3) (batch,height, width, color)\n",
    "- paths_to_tensor array of image path return the tensor of image in array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6670/6670 [01:03<00:00, 104.63it/s]\n",
      "100%|██████████| 835/835 [00:07<00:00, 119.19it/s]\n",
      "100%|██████████| 836/836 [00:07<00:00, 116.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True \n",
    "\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "#### While we have train the CNN found that it was overfitting by huge number where train accuracy was 50% and validation accuracy was only 18% in the 70 epochs. So to reduce the overfitting we try to do image augmentation.\n",
    "#### This helps prevent overfitting and helps the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # Rescaling factor\n",
    "        shear_range=0.2, # Shear angle in counter-clockwise direction in degrees\n",
    "        zoom_range=0.2, # Range for random zoom\n",
    "        horizontal_flip=True,# Randomly flip inputs horizontally.\n",
    "        fill_mode='nearest' #fill_mode is the strategy used for filling in newly created pixels,\n",
    "        ) \n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for testing\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is a generator that will read pictures found in subfolers of 'dogs/train', and indefinitely generate batches of augmented image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6670 images belonging to 133 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'Dog/train',  # this is the target directory\n",
    "        target_size=(224, 224),  # all images will be resized to 224 x 224\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical') # since we use categorical value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is a similar generator, for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 835 images belonging to 133 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'Dog/valid',\n",
    "        target_size=(224, 224),  # all images will be resized to 224 x 224\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> **GoogLeNet Inception Architecture** \n",
    "\n",
    "![GoogeLeNet inception Architecture](http://yeephycho.github.io/blog_img/GoogLeNet.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### It is generally difficult to decide which architecture will be good for the particular dataset it's most of the time trail and error if you are making CNN from scratch. Pre-train CNN with it's will give more accuracy in less iteration compare to training from scratch because network has to learn from beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook will only work on tensorflow not with theano** **Theano use channels as first whereas tensorflow uses channels as last**\n",
    "- Lets start with input tensor which will be: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = Input(shape = (224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## So let's start to make CNN for Our dataset which is Dog's dataset which contains 133 classes with total 8341"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Starting with CNN first layer from the diagram it would be `convolution` with `7 x 7` patch size and `stride` of `(2,2)` with input image of `224 x 244` followed by `BatchNormalization` for faster learning and higher overall accuracy. If want to know about [this](https://medium.com/deeper-learning/glossary-of-deep-learning-batch-normalisation-8266dcd2fa82) blog has good explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Conv2D(64,( 7, 7), strides=(2, 2), padding='same',activation='relu')(input)\n",
    "x = BatchNormalization()(x) #default axis is 3 is you are using theano it would be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `MaxPooling` with `3 x 3` with strides as `2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Next is `convolution` with `3 x 3` with stride 1 it has two convolution 3 x 3 reduce with 64 layers and 3 x 3 192 layers but as our dataset is way small compare to ImageNet we try to reduce the layer from 64 layers to 48 and 192 to 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Conv2D(48,(1,1),strides=(1,1),padding='same',activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64,(1,1),strides=(1,1),padding='same',activation='relu')(x)\n",
    "x = BatchNormalization()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception 3a type in the GoogLeNet Architecture\n",
    "- It is couple steps to complete this layers so making function so we can able to reuse it.\n",
    "- `1 x 1` 64 convolution layers followed by BatchNormalization\n",
    "- `3 x 3` 80 convolution layers where input is as out of `1 x 1` convolution layer followed by BatchNormalization\n",
    "- `5 x 5` 16 convolution layers where input is as out of `1 x 1` convolution layer followed by BatchNormalization\n",
    "- Last convolution layers is pooling which is 32 convolution layers with `1 x 1`\n",
    "- Merge ouptut of `1 x 1` , `3 x 3` and `5 x 5` with respect to last axis \n",
    "\n",
    "\n",
    "    So while calling function I would call as add_module(input, 64, 80, 16, 32, 32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_module(input,reduce_1, onex1, threex3, fivex5, pool):\n",
    "    #print(input.shape)\n",
    "    \n",
    "    Conv2D_reduce = Conv2D(reduce_1, (1,1), strides=(2,2), activation='relu', padding='same')(input)\n",
    "    Conv2D_reduce = BatchNormalization()(Conv2D_reduce)\n",
    "    #print(Conv2D_reduce.shape)\n",
    "    \n",
    "    Conv2D_1_1 = Conv2D(onex1, (1,1), activation='relu', padding='same')(input)\n",
    "    Conv2D_1_1 = BatchNormalization()(Conv2D_1_1)\n",
    "    #print(Conv2D_1_1.shape)\n",
    "    Conv2D_3_3 = Conv2D(threex3, (3,3),strides=(2,2), activation='relu', padding='same')(Conv2D_1_1)\n",
    "    Conv2D_3_3 = BatchNormalization()(Conv2D_3_3)\n",
    "    #print(Conv2D_3_3.shape)\n",
    "    Conv2D_5_5 = Conv2D(fivex5, (5,5),strides=(2,2), activation='relu', padding='same')(Conv2D_1_1)\n",
    "    Conv2D_5_5 = BatchNormalization()(Conv2D_5_5)\n",
    "    #print(Conv2D_5_5.shape)\n",
    "    \n",
    "    MaxPool2D_3_3 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(input)\n",
    "    #print(MaxPool2D_3_3.shape)\n",
    "    Cov2D_Pool = Conv2D(pool, (1,1), activation='relu', padding='same')(MaxPool2D_3_3)\n",
    "    Cov2D_Pool = BatchNormalization()(Cov2D_Pool)\n",
    "    #print(Cov2D_Pool.shape)\n",
    "    \n",
    "    concat = Concatenate(axis=-1)([Conv2D_reduce,Conv2D_3_3,Conv2D_5_5,Cov2D_Pool])\n",
    "    #print(concat.shape)\n",
    "    \n",
    "    return concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception 3b \n",
    "- It is couple steps to complete this layers.\n",
    "- `1 x 1` 80 convolution layers followed by BatchNormalization\n",
    "- `3 x 3` 16 convolution layers where input is as out of `1 x 1` convolution layer followed by BatchNormalization\n",
    "- `5 x 5` 48 convolution layers where input is as out of `1 x 1` convolution layer followed by BatchNormalization\n",
    "- Last convolution layers is pooling which is 64 convolution layers with `1 x 1`\n",
    "- Merge ouptut of `1 x 1` , `3 x 3` and `5 x 5` with respect to last axis\n",
    "\n",
    "    So while calling function I would call as add_module(input, 48, 80, 16, 48, 64) \n",
    "    \n",
    "And than adding maxpooling with `3 x 3` with strides of 3\n",
    "### So putting all together\n",
    "I am not using more complex architecture because that might overfitting model as per my dataset as shown in diagram because imagenet hase 1000 categorical images with each images have more than 1000 images of each category whereas in our case we have small dataset and for that the whole architecture implementation would overfit the model.\n",
    "#### Final layer I am using activation funtion as softmax with dense of 133 (num_classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(224,224,3))\n",
    "x = Conv2D(64,( 7, 7), strides=(2, 2), padding='same',activation='relu')(input)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)  \n",
    "x = Conv2D(48,(1,1),strides=(1,1),padding='same',activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64,(1,1),strides=(1,1),padding='same',activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = add_module(x, 64, 80, 16, 32, 32) \n",
    "# x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = add_module(x, 48, 80, 16, 48, 64)\n",
    "# x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "# x = add_module(x)\n",
    "# --- Last Layer --- \n",
    "\n",
    "\n",
    "# Now commes 3 level inception\n",
    "\n",
    "x = AveragePooling2D((7, 7), strides=(1, 1), padding='valid')(x) \n",
    "x = Dropout(0.5)(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='linear')(x)\n",
    "Output = Dense(133, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets make the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs= input, outputs = Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 112, 112, 64) 9472        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 112, 112, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 56, 56, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 56, 56, 48)   3120        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 56, 56, 48)   192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 56, 56, 64)   3136        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 56, 56, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 28, 28, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 28, 28, 80)   5200        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 28, 28, 80)   320         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 14, 14, 64)   0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 14, 14, 64)   4160        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 14, 14, 16)   11536       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 14, 14, 32)   64032       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 14, 14, 32)   2080        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 14, 14, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 14, 14, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 14, 14, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 14, 14, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 14, 14, 144)  0           batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 14, 14, 80)   11600       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 14, 14, 80)   320         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 7, 7, 144)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 7, 7, 48)     6960        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 7, 7, 16)     11536       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 7, 7, 48)     96048       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 7, 7, 64)     9280        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 7, 7, 48)     192         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 7, 7, 16)     64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 7, 7, 48)     192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 7, 7, 64)     256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 7, 7, 176)    0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 176)    0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 1, 176)    0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 176)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         181248      global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 133)          136325      dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 558,357\n",
      "Trainable params: 557,045\n",
      "Non-trainable params: 1,312\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traning Starts\n",
    " We have setup VM with two GPU attach to instance so we are going to use parallel model of gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "parallel_model = multi_gpu_model(model, gpus=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers selecting is very important to get the model good accuracy as per the paper I am using SGD optimizers as it gives the best result incase of the imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parallel_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First try to run for 20 epochs it takes ~ 145s for 2 GPUs K-80 tesla with 16 memory and where enresult loss_function for training is 3.8295 and for validation is 4.2274.\n",
    "#### Run only 20 epochs to check the model is working.\n",
    "#### Still running on 40 more epochs but got validation_accuracy of 12% and training_accuracy as 20%\n",
    "#### Reasons for running less epochs to check if model is overfitting or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commented out because I don't want to run my training by mistake in this file because it takes forever and will not able to do anything else in this file.\n",
    "![First-20-Epochs](https://github.com/vishal6557/ADS/blob/master/Screen%20Shot%202018-04-23%20at%204.01.17%20AM.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parallel_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=6670 // batch_size,\n",
    "        epochs=20,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=835 // batch_size)\n",
    "model.save_weights('testing_fina1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model in Json Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"final_ model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model from json with its weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('final_ model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"testing_fina17.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget to compile the model before using it or else it will give error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parallel_model = multi_gpu_model(loaded_model, gpus=2)\n",
    "loaded_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a generator to get the accuracy and from the model you trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6670 images belonging to 133 classes.\n"
     ]
    }
   ],
   "source": [
    "generator = train_datagen.flow_from_directory(\n",
    "        'Dog/train',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,  # this means our generator will only yield batches of data, no labels\n",
    "        shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = loaded_model.evaluate_generator(validation_generator, 800/16, workers=12)\n",
    "\n",
    "scores = loaded_model.predict_generator(validation_generator, 800/16, workers=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 8  Total:  835\n",
      "Loss:  3.401902123070753 Accuracy:  16.95965176890109 %\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "for i, n in enumerate(validation_generator.filenames):\n",
    "       if \"Affenpinscher\" in n and scores[i][0] <= 0.5:\n",
    "        correct += 1\n",
    "\n",
    "print(\"Correct:\", correct, \" Total: \", len(validation_generator.filenames))\n",
    "print(\"Loss: \", score[0], \"Accuracy: \", score[1]*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This will give you the accuracy of the dog bread we search as `_Affenpinscher_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying CIFAR-10 for GoogleNet\n",
    "- As the image size is small in CIFAR-10 i.e. 32x32 we are using one layer as mention in figure.\n",
    "\n",
    "![googlenet](https://qph.fs.quoracdn.net/main-qimg-1593dbc4944be77ade976bbb8e1dc0b2-c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "\n",
    "Conv2D_1 = Conv2D(64, (3,3), activation='relu', padding='same')(input)\n",
    "MaxPool2D_1 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(Conv2D_1)\n",
    "BatchNorm_1 = BatchNormalization()(MaxPool2D_1)\n",
    "\n",
    "Module_1 = add_module(BatchNorm_1, 16, 16, 16, 16,16)\n",
    "Module_1 = add_module(Module_1,16, 16, 16, 16,16)\n",
    "\n",
    "Output = Flatten()(Module_1)\n",
    "Output = Dense(num_classes, activation='softmax')(Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 32, 32, 64)   1792        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling2D) (None, 16, 16, 64)   0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 16, 16, 64)   256         max_pooling2d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 16, 16, 16)   1040        batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 16, 16, 16)   64          conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 16)     1040        batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 16)     2320        batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 16)     6416        batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling2D) (None, 8, 8, 64)     0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 16)     64          conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 8, 16)     64          conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 8, 8, 16)     64          conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 16)     1040        max_pooling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 8, 8, 64)     0           batch_normalization_72[0][0]     \n",
      "                                                                 batch_normalization_74[0][0]     \n",
      "                                                                 batch_normalization_75[0][0]     \n",
      "                                                                 conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 16)     1040        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 16)     64          conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 4, 4, 16)     1040        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 4, 4, 16)     2320        batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 4, 4, 16)     6416        batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling2D) (None, 4, 4, 64)     0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 4, 4, 16)     64          conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 4, 4, 16)     64          conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 4, 4, 16)     64          conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 4, 4, 16)     1040        max_pooling2d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 4, 4, 64)     0           batch_normalization_76[0][0]     \n",
      "                                                                 batch_normalization_78[0][0]     \n",
      "                                                                 batch_normalization_79[0][0]     \n",
      "                                                                 conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1024)         0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           10250       flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 36,522\n",
      "Trainable params: 36,138\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[Output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parallel_model = multi_gpu_model(model, gpus=2)\n",
    "RMsprop=RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "parallel_model.compile(loss='categorical_crossentropy', optimizer=RMsprop, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 52s 1ms/step - loss: 1.7030 - acc: 0.4012 - val_loss: 1.5079 - val_acc: 0.4636\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 49s 988us/step - loss: 1.4144 - acc: 0.4974 - val_loss: 1.3259 - val_acc: 0.5272\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 50s 992us/step - loss: 1.2600 - acc: 0.5505 - val_loss: 1.2323 - val_acc: 0.5544\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 50s 991us/step - loss: 1.1661 - acc: 0.5866 - val_loss: 1.2349 - val_acc: 0.5603\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 50s 991us/step - loss: 1.0984 - acc: 0.6126 - val_loss: 1.1461 - val_acc: 0.5873\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 50s 991us/step - loss: 1.0439 - acc: 0.6332 - val_loss: 1.1066 - val_acc: 0.6004\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 49s 989us/step - loss: 0.9957 - acc: 0.6478 - val_loss: 1.0820 - val_acc: 0.6116\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 49s 988us/step - loss: 0.9565 - acc: 0.6640 - val_loss: 1.0496 - val_acc: 0.6222\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 50s 990us/step - loss: 0.9220 - acc: 0.6759 - val_loss: 1.0571 - val_acc: 0.6264\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 49s 989us/step - loss: 0.8907 - acc: 0.6888 - val_loss: 1.0301 - val_acc: 0.6337\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 49s 980us/step - loss: 0.8649 - acc: 0.6984 - val_loss: 1.0468 - val_acc: 0.6350\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 48s 963us/step - loss: 0.8382 - acc: 0.7060 - val_loss: 1.0742 - val_acc: 0.6246\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 49s 973us/step - loss: 0.8163 - acc: 0.7177 - val_loss: 0.9941 - val_acc: 0.6518\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 49s 983us/step - loss: 0.7951 - acc: 0.7236 - val_loss: 0.9804 - val_acc: 0.6591\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 49s 987us/step - loss: 0.7775 - acc: 0.7300 - val_loss: 1.0012 - val_acc: 0.6540\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 48s 967us/step - loss: 0.7636 - acc: 0.7355 - val_loss: 0.9677 - val_acc: 0.6620\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 48s 960us/step - loss: 0.7456 - acc: 0.7421 - val_loss: 1.0059 - val_acc: 0.6613\n",
      "Epoch 18/50\n",
      " 9952/50000 [====>.........................] - ETA: 35s - loss: 0.7262 - acc: 0.7468"
     ]
    }
   ],
   "source": [
    "parallel_model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This I think stop because I keep running my VM and sleep my laptop so it didnot autosave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.57%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test,  y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Challenges we face doing this project so might help to others who want to do similar type of Project :\n",
    "\n",
    "-  Make sure you have good computation power with atleast two GPU.\n",
    "\n",
    "- Trying to run model in parallel so it takes less time to train and test.\n",
    "\n",
    "- Start with simple architecture try to run 30-40 echos first and check the model is overfitting or not If it is overfitting than you don't need to break the trainning. \n",
    "\n",
    "- Try to take best weight in every epochs I know it kind of tricky in parallel model but you can do this by following the links \n",
    "\n",
    "- Use jupyter notebook in background using nohup the documentation for that in this [link](https://hackernoon.com/aws-ec2-part-4-starting-a-jupyter-ipython-notebook-server-on-aws-549d87a55ba9)\n",
    "\n",
    "- If you to try notebook which contain code of THEANO you can do it in 3 easy steps:\n",
    "    1. First [install](http://deeplearning.net/software/theano/install.html) theano with GPU.\n",
    "    \n",
    "    2. `nano ~/.keras/keras.json ` Change following:\n",
    "    \n",
    "           {\n",
    "                \"image_dim_ordering\": \"th\",\n",
    "                \"backend\": \"theano\",\n",
    "                \"image_data_format\": \"channels_first\"\n",
    "            } \n",
    "       \n",
    "    - Restart the jupyter notebook\n",
    "- My model was overfitting and I was not able to figureout what it was overfitting so first thing I tried is Image Augmentation , than change the learning rate than change the layers and than so on. It's I guess  **trail and error**  methods and In my case each epochs takes around 160-180s with two K-80 tesla GPU it was time cosuming but good way to learn. Shoud have patience.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of this project itself is licensed under the [Creative Commons Attribution 3.0 United States License](http://creativecommons.org/licenses/by/3.0/us/), and the underlying source code used to format and display that content is licensed under the [MIT LICENSE](https://github.com/vishal6557/ADS/blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Citations\n",
    "\n",
    "<a id='google-net'>\n",
    "[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. \"ImageNet Classification with Deep Convolutional Neural Networks.\" NIPS 2012\n",
    "<br>\n",
    "\n",
    "<a id='inception-v1-paper'>\n",
    "[2] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n",
    "Dragomir Anguelov, Dumitru Erhan, Andrew Rabinovich.\n",
    "\"Going Deeper with Convolutions.\" CVPR 2015.\n",
    "<br>\n",
    "\n",
    "<a id='vgg-paper'>\n",
    "[3] Karen Simonyan and Andrew Zisserman. \"Very Deep Convolutional Networks for Large-Scale Image Recognition.\" ICLR 2015\n",
    "<br>\n",
    "\n",
    "<a id='resnet-cvpr'>\n",
    "[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Deep Residual Learning for Image Recognition.\" CVPR 2016.\n",
    "<br>\n",
    "\n",
    "<a id='resnet-eccv'>\n",
    "[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Identity Mappings in Deep Residual Networks.\" ECCV 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "[1] [GoogleNet in Keras](http://joelouismarino.github.io/blog_posts/blog_googlenet_keras.html) for understanding of GoogleNet Architecture.\n",
    "\n",
    "[2] [Keras Documentation](https://keras.io/) for how to use Keras\n",
    "\n",
    "[3] [Convolution Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/) for understanding how CNN works\n",
    "\n",
    "[4] [How convolution neural network Works](https://www.youtube.com/watch?v=FmpDIaiMIeA&t=634s)\n",
    "\n",
    "[5] [Dog breed classification with Keras](http://machinememos.com/python/keras/artificial%20intelligence/machine%20learning/transfer%20learning/dog%20breed/neural%20networks/convolutional%20neural%20network/tensorflow/image%20classification/imagenet/2017/07/11/dog-breed-image-classification.html)\n",
    "\n",
    "[6] Keras blog [Image Augmentation](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    "\n",
    "[7] [Image Datagenertor-Methods](https://keras.io/preprocessing/image/#imagedatagenerator-methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
